{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "669ad4bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\SonerAstan\\OneDrive - Schwarz IT\\DHBW-Heilbronn\\Auslandssemester\\Semester-Kurse\\Data-Mining-and-Knowledge-Discovery\\Project\\data_mining_project\\venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import nltk\n",
    "import joblib\n",
    "import numpy as np\n",
    "from scipy.sparse import save_npz, load_npz\n",
    "from sklearn.model_selection import train_test_split\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "\n",
    "# Setup src imports\n",
    "sys.path.append(os.path.abspath(os.path.join(os.getcwd(), '..', 'src')))\n",
    "from data_loader import load_authorship_dataset\n",
    "from features import extract_tfidf_double_norm\n",
    "from bert_features import extract_bert_embeddings\n",
    "from logger import get_logger"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f533dfb9",
   "metadata": {},
   "source": [
    "# Init logger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "20db1db8",
   "metadata": {},
   "outputs": [],
   "source": [
    "logger = get_logger(\"preprocessing\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ada1c1e",
   "metadata": {},
   "source": [
    "# NLTK Stopwords (ensure available)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "637b7d4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download(\"stopwords\", quiet=True)\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8ada5c2",
   "metadata": {},
   "source": [
    "# Paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f1a73057",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_RAW_PATH = \"./../data/raw/dataset_authorship\"\n",
    "DATA_PROCESSED_PATH = \"./../data/processed\"\n",
    "DATA_TRAIN_PATH = \"./../data/train\"\n",
    "DATA_TEST_PATH = \"./../data/test\"\n",
    "os.makedirs(DATA_PROCESSED_PATH, exist_ok=True)\n",
    "os.makedirs(DATA_TRAIN_PATH, exist_ok=True)\n",
    "os.makedirs(DATA_TEST_PATH, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "036b982e",
   "metadata": {},
   "source": [
    "# Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bf11ac6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-04-21 13:09:32 [INFO] 📂 Found 30 author folders.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-04-21 13:09:34 [INFO] 🔢 Encoded 30 unique authors.\n",
      "2025-04-21 13:09:34 [INFO] ✅ Loaded 1200 documents from dataset.\n"
     ]
    }
   ],
   "source": [
    "df, label_encoder = load_authorship_dataset(DATA_RAW_PATH)\n",
    "texts = df[\"text\"].tolist()\n",
    "labels = df[\"label\"].values\n",
    "turkish_stopwords = stopwords.words(\"turkish\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a82a087",
   "metadata": {},
   "source": [
    "# TF-IDF config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d270ea84",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_configs = [\n",
    "    {\"name\": \"tfidf_word_1gram\", \"analyzer\": \"word\", \"ngram_range\": (1, 1), \"stopwords\": turkish_stopwords},\n",
    "    {\"name\": \"tfidf_word_2gram\", \"analyzer\": \"word\", \"ngram_range\": (2, 2), \"stopwords\": turkish_stopwords},\n",
    "    {\"name\": \"tfidf_word_3gram\", \"analyzer\": \"word\", \"ngram_range\": (3, 3), \"stopwords\": turkish_stopwords},\n",
    "    {\"name\": \"tfidf_char_2gram\", \"analyzer\": \"char\", \"ngram_range\": (2, 2), \"stopwords\": None},\n",
    "    {\"name\": \"tfidf_char_3gram\", \"analyzer\": \"char\", \"ngram_range\": (3, 3), \"stopwords\": None},\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e44a81e",
   "metadata": {},
   "source": [
    "# Extract and save TF-IDF and BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fa7d8810",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-04-21 13:09:34 [INFO] 🚀 Starting parallel preprocessing...\n",
      "2025-04-21 13:09:34 [INFO] 🧮 TF-IDF: tfidf_word_1gram started\n",
      "2025-04-21 13:09:34 [INFO] 🧮 TF-IDF: tfidf_word_2gram started\n",
      "2025-04-21 13:09:34 [INFO] 📥 Loading tokenizer and model...\n",
      "2025-04-21 13:09:34 [INFO] 🧮 TF-IDF: tfidf_word_3gram started\n",
      "2025-04-21 13:09:34 [INFO] 🧮 TF-IDF: tfidf_char_2gram started\n",
      "2025-04-21 13:09:34 [INFO] 🧮 TF-IDF: tfidf_char_3gram started\n",
      "2025-04-21 13:09:51 [INFO] ✅ Saved tfidf_word_1gram with shape: (1200, 10000)\n",
      "2025-04-21 13:09:53 [INFO] ✅ Saved tfidf_word_2gram with shape: (1200, 10000)\n",
      "2025-04-21 13:09:54 [INFO] ✅ Saved tfidf_word_3gram with shape: (1200, 10000)\n",
      "2025-04-21 13:09:54 [INFO] ✅ Saved tfidf_char_2gram with shape: (1200, 2658)\n",
      "2025-04-21 13:09:56 [INFO] ✅ Saved tfidf_char_3gram with shape: (1200, 10000)\n",
      "2025-04-21 13:10:13 [INFO] 📦 Using device: cpu\n",
      "2025-04-21 13:10:13 [INFO] 📂 Loading dataset...\n",
      "2025-04-21 13:10:13 [INFO] 📂 Found 30 author folders.\n",
      "2025-04-21 13:10:14 [INFO] 🔢 Encoded 30 unique authors.\n",
      "2025-04-21 13:10:14 [INFO] ✅ Loaded 1200 documents from dataset.\n",
      "2025-04-21 13:10:14 [INFO] 🚀 Generating embeddings in batches...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "BERT batches: 100%|██████████| 75/75 [07:35<00:00,  6.07s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-04-21 13:17:49 [INFO] ✅ Embeddings shape: (1200, 768)\n",
      "2025-04-21 13:17:49 [INFO] 💾 Embeddings and labels saved to: ./../data/processed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-04-21 13:17:49 [INFO] 🏁 All feature sets completed.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# === TF-IDF Function ===\n",
    "def process_tfidf_config(config):\n",
    "    logger.info(f\"🧮 TF-IDF: {config['name']} started\")\n",
    "    X, vectorizer, idf = extract_tfidf_double_norm(\n",
    "        texts,\n",
    "        ngram_range=config[\"ngram_range\"],\n",
    "        analyzer=config[\"analyzer\"],\n",
    "        stopword_list=config[\"stopwords\"],\n",
    "        max_features=10000\n",
    "    )\n",
    "    save_npz(f\"{DATA_PROCESSED_PATH}/X_{config['name']}.npz\", X)\n",
    "    joblib.dump(vectorizer, f\"{DATA_PROCESSED_PATH}/vectorizer_{config['name']}.pkl\")\n",
    "    np.save(f\"{DATA_PROCESSED_PATH}/idf_{config['name']}.npy\", idf)\n",
    "    logger.info(f\"✅ Saved {config['name']} with shape: {X.shape}\")\n",
    "\n",
    "# === BERT Function ===\n",
    "def run_bert():\n",
    "    extract_bert_embeddings(\n",
    "        dataset_path=DATA_RAW_PATH,\n",
    "        save_dir=DATA_PROCESSED_PATH,\n",
    "        batch_size=16\n",
    "    )\n",
    "\n",
    "# === Run in parallel ===\n",
    "logger.info(\"🚀 Starting parallel preprocessing...\")\n",
    "with ThreadPoolExecutor(max_workers=6) as executor:\n",
    "    # Submit: TF-IDF + BERT\n",
    "    futures = (\n",
    "        [executor.submit(process_tfidf_config, config) for config in feature_configs]\n",
    "        + [executor.submit(run_bert)]\n",
    "    )\n",
    "\n",
    "    for f in futures:\n",
    "        f.result()\n",
    "\n",
    "logger.info(\"🏁 All feature sets completed.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "233f775d",
   "metadata": {},
   "source": [
    "# Split helper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d47724f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_and_save(X, y, name, is_sparse=True):\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, test_size=0.2, stratify=y, random_state=42\n",
    "    )\n",
    "\n",
    "    if is_sparse:\n",
    "        save_npz(f\"{DATA_TRAIN_PATH}/X_{name}.npz\", X_train)\n",
    "        save_npz(f\"{DATA_TEST_PATH}/X_{name}.npz\", X_test)\n",
    "    else:\n",
    "        np.save(f\"{DATA_TRAIN_PATH}/X_{name}.npy\", X_train)\n",
    "        np.save(f\"{DATA_TEST_PATH}/X_{name}.npy\", X_test)\n",
    "\n",
    "    np.save(f\"{DATA_TRAIN_PATH}/y_{name}.npy\", y_train)\n",
    "    np.save(f\"{DATA_TEST_PATH}/y_{name}.npy\", y_test)\n",
    "\n",
    "    logger.info(f\"📊 {name} split → train: {X_train.shape}, test: {X_test.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a2e806e",
   "metadata": {},
   "source": [
    "# Split TF-IDF features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cf2c9a3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-04-21 13:17:50 [INFO] 📊 tfidf_word_1gram split → train: (960, 10000), test: (240, 10000)\n",
      "2025-04-21 13:17:50 [INFO] 📊 tfidf_word_2gram split → train: (960, 10000), test: (240, 10000)\n",
      "2025-04-21 13:17:50 [INFO] 📊 tfidf_word_3gram split → train: (960, 10000), test: (240, 10000)\n",
      "2025-04-21 13:17:50 [INFO] 📊 tfidf_char_2gram split → train: (960, 2658), test: (240, 2658)\n",
      "2025-04-21 13:17:51 [INFO] 📊 tfidf_char_3gram split → train: (960, 10000), test: (240, 10000)\n"
     ]
    }
   ],
   "source": [
    "for config in feature_configs:\n",
    "    name = config[\"name\"]\n",
    "    X = load_npz(f\"{DATA_PROCESSED_PATH}/X_{name}.npz\")\n",
    "    split_and_save(X, labels, name, is_sparse=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ff651f4",
   "metadata": {},
   "source": [
    "# Split BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d44ede16",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-04-21 13:17:51 [INFO] 📊 bert split → train: (960, 768), test: (240, 768)\n"
     ]
    }
   ],
   "source": [
    "X_bert = np.load(f\"{DATA_PROCESSED_PATH}/X_bert_embeddings.npy\")\n",
    "y_bert = np.load(f\"{DATA_PROCESSED_PATH}/y_bert.npy\")\n",
    "split_and_save(X_bert, y_bert, \"bert\", is_sparse=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
