{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "669ad4bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Error loading stopwords: <urlopen error [Errno 11001]\n",
      "[nltk_data]     getaddrinfo failed>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e6683f39",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add src to path\n",
    "import sys\n",
    "import os\n",
    "sys.path.append(os.path.abspath(os.path.join(os.getcwd(), '..', 'src')))\n",
    "\n",
    "from data_loader import load_authorship_dataset\n",
    "from features import extract_tfidf_double_norm\n",
    "from bert_features import extract_bert_embeddings\n",
    "from scipy.sparse import save_npz\n",
    "import joblib\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "637b7d4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_RAW_PATH = \"./../data/raw/dataset_authorship\"\n",
    "DATA_PROCESSED_PATH = \"./../data/processed\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "bf11ac6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing: word_1gram ...\n",
      "âœ” Saved word_1gram with shape (1200, 10000)\n",
      "Processing: word_2gram ...\n",
      "âœ” Saved word_2gram with shape (1200, 10000)\n",
      "Processing: word_3gram ...\n",
      "âœ” Saved word_3gram with shape (1200, 10000)\n",
      "Processing: char_2gram ...\n",
      "âœ” Saved char_2gram with shape (1200, 2658)\n",
      "Processing: char_3gram ...\n",
      "âœ” Saved char_3gram with shape (1200, 10000)\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "\n",
    "\n",
    "# Setup\n",
    "df, label_encoder = load_authorship_dataset(DATA_RAW_PATH)\n",
    "texts = df[\"text\"].tolist()\n",
    "turkish_stopwords = stopwords.words(\"turkish\")\n",
    "os.makedirs(\"./../data/processed\", exist_ok=True)\n",
    "\n",
    "# Feature configuration\n",
    "feature_configs = [\n",
    "    {\"name\": \"word_1gram\", \"analyzer\": \"word\", \"ngram_range\": (1, 1), \"stopwords\": turkish_stopwords},\n",
    "    {\"name\": \"word_2gram\", \"analyzer\": \"word\", \"ngram_range\": (2, 2), \"stopwords\": turkish_stopwords},\n",
    "    {\"name\": \"word_3gram\", \"analyzer\": \"word\", \"ngram_range\": (3, 3), \"stopwords\": turkish_stopwords},\n",
    "    {\"name\": \"char_2gram\", \"analyzer\": \"char\", \"ngram_range\": (2, 2), \"stopwords\": None},\n",
    "    {\"name\": \"char_3gram\", \"analyzer\": \"char\", \"ngram_range\": (3, 3), \"stopwords\": None},\n",
    "]\n",
    "\n",
    "# Processing loop\n",
    "for config in feature_configs:\n",
    "    print(f\"Processing: {config['name']} ...\")\n",
    "\n",
    "    X, vectorizer, idf = extract_tfidf_double_norm(\n",
    "        texts,\n",
    "        ngram_range=config[\"ngram_range\"],\n",
    "        analyzer=config[\"analyzer\"],\n",
    "        stopword_list=config[\"stopwords\"],\n",
    "        max_features=10000\n",
    "    )\n",
    "\n",
    "    save_npz(f\"{DATA_PROCESSED_PATH}/X_tfidf_{config['name']}.npz\", X)\n",
    "    joblib.dump(vectorizer, f\"{DATA_PROCESSED_PATH}/vectorizer_{config['name']}.pkl\")\n",
    "    np.save(f\"{DATA_PROCESSED_PATH}/idf_{config['name']}.npy\", idf)\n",
    "\n",
    "    print(f\"âœ” Saved {config['name']} with shape {X.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "26dfefc1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“¥ Loading tokenizer and model...\n",
      "ðŸ“‚ Loading dataset...\n",
      "ðŸš€ Generating embeddings...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating BERT embeddings: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1200/1200 [11:53<00:00,  1.68it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Embeddings shape: (1200, 768)\n",
      "ðŸ’¾ Saved to ./../data/processed\n"
     ]
    }
   ],
   "source": [
    "extract_bert_embeddings(\n",
    "    dataset_path=DATA_RAW_PATH,\n",
    "    save_dir=DATA_PROCESSED_PATH\n",
    ")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
